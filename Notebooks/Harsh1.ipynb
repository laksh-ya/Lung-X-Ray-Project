{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92cd56c5-84c9-4603-98c8-a43a282a0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 954ms/step\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step \n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thebe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.7627 - loss: 7.2611 - val_accuracy: 0.8782 - val_loss: 5.2629 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8672 - loss: 4.7790 - val_accuracy: 0.8647 - val_loss: 3.2884 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8918 - loss: 2.9257 - val_accuracy: 0.8632 - val_loss: 2.1215 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9292 - loss: 1.8522 - val_accuracy: 0.8722 - val_loss: 1.5712 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9265 - loss: 1.3815 - val_accuracy: 0.8782 - val_loss: 1.2980 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9352 - loss: 1.1236 - val_accuracy: 0.8827 - val_loss: 1.1691 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9265 - loss: 1.0209 - val_accuracy: 0.8617 - val_loss: 1.0838 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9337 - loss: 0.9216 - val_accuracy: 0.8707 - val_loss: 1.0087 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9266 - loss: 0.8600 - val_accuracy: 0.8842 - val_loss: 0.9439 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9439 - loss: 0.7408 - val_accuracy: 0.8797 - val_loss: 0.8714 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9359 - loss: 0.7283 - val_accuracy: 0.8887 - val_loss: 0.8614 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9268 - loss: 0.7206 - val_accuracy: 0.8737 - val_loss: 0.8432 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9355 - loss: 0.6584 - val_accuracy: 0.8692 - val_loss: 0.8130 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9222 - loss: 0.6594 - val_accuracy: 0.8722 - val_loss: 0.8023 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9271 - loss: 0.6303 - val_accuracy: 0.8722 - val_loss: 0.7841 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9362 - loss: 0.6238 - val_accuracy: 0.8752 - val_loss: 0.7351 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9180 - loss: 0.6664 - val_accuracy: 0.8692 - val_loss: 0.7277 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9311 - loss: 0.5905 - val_accuracy: 0.8827 - val_loss: 0.7553 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9266 - loss: 0.5941 - val_accuracy: 0.8782 - val_loss: 0.7347 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9333 - loss: 0.5701 - val_accuracy: 0.8767 - val_loss: 0.7280 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9316 - loss: 0.5656 - val_accuracy: 0.8842 - val_loss: 0.5955 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9609 - loss: 0.3884 - val_accuracy: 0.8797 - val_loss: 0.5812 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9511 - loss: 0.4150 - val_accuracy: 0.8662 - val_loss: 0.6733 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9567 - loss: 0.4046 - val_accuracy: 0.8662 - val_loss: 0.6716 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9554 - loss: 0.4100 - val_accuracy: 0.8617 - val_loss: 0.6855 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9583 - loss: 0.3934 - val_accuracy: 0.8767 - val_loss: 0.5639 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9795 - loss: 0.2895 - val_accuracy: 0.8827 - val_loss: 0.5457 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9851 - loss: 0.2448 - val_accuracy: 0.8857 - val_loss: 0.5349 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9872 - loss: 0.2079 - val_accuracy: 0.8842 - val_loss: 0.5102 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9813 - loss: 0.2291 - val_accuracy: 0.8812 - val_loss: 0.5287 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9794 - loss: 0.2376 - val_accuracy: 0.8872 - val_loss: 0.5671 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9702 - loss: 0.2686 - val_accuracy: 0.8812 - val_loss: 0.5860 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9841 - loss: 0.2412 - val_accuracy: 0.8872 - val_loss: 0.5515 - learning_rate: 1.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9915 - loss: 0.2056 - val_accuracy: 0.8872 - val_loss: 0.5069 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9949 - loss: 0.1807 - val_accuracy: 0.8857 - val_loss: 0.4995 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9916 - loss: 0.1657 - val_accuracy: 0.8917 - val_loss: 0.4770 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9955 - loss: 0.1498 - val_accuracy: 0.8857 - val_loss: 0.4779 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9959 - loss: 0.1388 - val_accuracy: 0.8797 - val_loss: 0.4727 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9950 - loss: 0.1310 - val_accuracy: 0.8827 - val_loss: 0.4760 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9959 - loss: 0.1246 - val_accuracy: 0.8842 - val_loss: 0.4585 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9961 - loss: 0.1165 - val_accuracy: 0.8917 - val_loss: 0.4447 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9954 - loss: 0.1169 - val_accuracy: 0.8887 - val_loss: 0.4546 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9931 - loss: 0.1188 - val_accuracy: 0.8842 - val_loss: 0.4364 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9883 - loss: 0.1324 - val_accuracy: 0.8872 - val_loss: 0.4513 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9909 - loss: 0.1326 - val_accuracy: 0.8917 - val_loss: 0.4928 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.1386 - val_accuracy: 0.8782 - val_loss: 0.5211 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.9888 - loss: 0.1377 - val_accuracy: 0.8872 - val_loss: 0.4891 - learning_rate: 6.2500e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9938 - loss: 0.1250 - val_accuracy: 0.8872 - val_loss: 0.4747 - learning_rate: 6.2500e-05\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91       418\n",
      "           1       0.87      0.81      0.84       247\n",
      "\n",
      "    accuracy                           0.88       665\n",
      "   macro avg       0.88      0.87      0.87       665\n",
      "weighted avg       0.88      0.88      0.88       665\n",
      "\n",
      "Accuracy: 0.8842105263157894\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thebe\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lime\\lime_tabular.py:372: UserWarning: \n",
      "                    Prediction probabilties do not sum to 1, and\n",
      "                    thus does not constitute a probability space.\n",
      "                    Check that you classifier outputs probabilities\n",
      "                    (Not log probabilities, or actual class predictions).\n",
      "                    \n",
      "  warnings.warn(\"\"\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# LIME Explanation\u001b[39;00m\n\u001b[0;32m    124\u001b[0m explainer \u001b[38;5;241m=\u001b[39m LimeTabularExplainer(X_train_features, feature_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X_train_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])], class_names\u001b[38;5;241m=\u001b[39mlabel_encoder\u001b[38;5;241m.\u001b[39mclasses_, discretize_continuous\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 125\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m exp\u001b[38;5;241m.\u001b[39mshow_in_notebook()\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Plot Training & Validation Graphs\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lime\\lime_tabular.py:452\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    448\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m    450\u001b[0m     (ret_exp\u001b[38;5;241m.\u001b[39mintercept[label],\n\u001b[0;32m    451\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mlocal_exp[label],\n\u001b[1;32m--> 452\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mscore, ret_exp\u001b[38;5;241m.\u001b[39mlocal_pred) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance_with_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43myss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_regressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    462\u001b[0m     ret_exp\u001b[38;5;241m.\u001b[39mintercept[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ret_exp\u001b[38;5;241m.\u001b[39mintercept[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lime\\lime_base.py:182\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[1;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Takes perturbed data, labels and distances, returns explanation.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    local_pred is the prediction of the explanation model on the original instance\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    181\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_fn(distances)\n\u001b[1;32m--> 182\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m \u001b[43mneighborhood_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    183\u001b[0m used_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection(neighborhood_data,\n\u001b[0;32m    184\u001b[0m                                        labels_column,\n\u001b[0;32m    185\u001b[0m                                        weights,\n\u001b[0;32m    186\u001b[0m                                        num_features,\n\u001b[0;32m    187\u001b[0m                                        feature_selection)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_regressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Data Loading with Augmentation\n",
    "base_path = '../Database/'\n",
    "data, labels = [], []\n",
    "\n",
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=30, width_shift_range=0.2, height_shift_range=0.2,\n",
    "    shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2], fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Load Data\n",
    "def load_images_from_folder(folder, label):\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    for img_name in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "load_images_from_folder(\"Normal\", \"Normal\")\n",
    "for folder in [\"Lung_Opacity\", \"Viral Pneumonia\"]:\n",
    "    load_images_from_folder(folder, \"Lung_Disease\")\n",
    "\n",
    "data = np.array(data).astype('float32') / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded\n",
    ")\n",
    "\n",
    "# Extract HOG Features\n",
    "def extract_hog_features(images):\n",
    "    return np.array([hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                          cells_per_block=(2, 2), visualize=False) for img in images])\n",
    "\n",
    "X_train_hog = extract_hog_features(X_train)\n",
    "X_test_hog = extract_hog_features(X_test)\n",
    "\n",
    "# CNN Feature Extraction (ResNet50)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "def extract_cnn_features(images):\n",
    "    images = np.repeat(images[..., np.newaxis], 3, axis=-1)\n",
    "    return feature_extractor.predict(images)\n",
    "\n",
    "X_train_cnn = extract_cnn_features(X_train)\n",
    "X_test_cnn = extract_cnn_features(X_test)\n",
    "\n",
    "# Combine CNN + HOG Features\n",
    "X_train_features = np.hstack((X_train_cnn, X_train_hog))\n",
    "X_test_features = np.hstack((X_test_cnn, X_test_hog))\n",
    "\n",
    "# Handle Imbalance Using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_features, y_train = smote.fit_resample(X_train_features, y_train)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_features = scaler.fit_transform(X_train_features)\n",
    "X_test_features = scaler.transform(X_test_features)\n",
    "\n",
    "# Model Training\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(256, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train_features.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning Rate Scheduler & Early Stopping\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_features, y_train, validation_data=(X_test_features, y_test),\n",
    "                    epochs=50, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "# Evaluation\n",
    "y_pred = (model.predict(X_test_features) > 0.5).astype(\"int32\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# LIME Explanation\n",
    "explainer = LimeTabularExplainer(X_train_features, feature_names=[\"Feature \" + str(i) for i in range(X_train_features.shape[1])], class_names=label_encoder.classes_, discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_test_features[0], lambda x: model.predict(x), num_features=10)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# Plot Training & Validation Graphs\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59a961-22ac-4345-a992-73ed692177f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
