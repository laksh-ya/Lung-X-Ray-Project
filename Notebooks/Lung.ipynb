{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f204ca2-b18c-4d67-bd6d-fba0088ff56d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114026,
     "status": "ok",
     "timestamp": 1738012106640,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "0f204ca2-b18c-4d67-bd6d-fba0088ff56d",
    "outputId": "4bd08d2a-2e1d-49be-ba7d-d34c48c6e3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in d:\\anaconda\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\aakansh\\appdata\\roaming\\python\\python312\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load images and labels\n",
    "base_path = 'C:/Users/Aakansh/Lung Classification/Lung-X-Ray-Project/Database/'\n",
    "categories = ['Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
    "data, labels = [], []\n",
    "\n",
    "for category in categories:\n",
    "    path = os.path.join(base_path, category)\n",
    "    label = category\n",
    "    for img_name in os.listdir(path):\n",
    "        img_path = os.path.join(path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
    "        img = cv2.resize(img, (128, 128))  # Resize to a fixed size\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert to arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JlYXJjrhZ9lr",
   "metadata": {
    "id": "JlYXJjrhZ9lr"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab40687-a7d2-4a15-8e37-084a90052856",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1738012106641,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "6ab40687-a7d2-4a15-8e37-084a90052856"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Histogram equalization\n",
    "    hist_eq = cv2.equalizeHist(image)\n",
    "\n",
    "    # Sharpening using a kernel\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "    sharpened = cv2.filter2D(hist_eq, -1, kernel)\n",
    "\n",
    "    return sharpened\n",
    "\n",
    "X_train_processed = np.array([preprocess_image(img) for img in X_train])\n",
    "X_test_processed = np.array([preprocess_image(img) for img in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8b622e-83a5-472c-b711-d6344fed303b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104350,
     "status": "ok",
     "timestamp": 1738012210985,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "7e8b622e-83a5-472c-b711-d6344fed303b",
    "outputId": "3b35fa8d-4a6c-403f-a09a-13b3dfcd3ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mahotas in d:\\anaconda\\lib\\site-packages (1.4.18)\n",
      "Requirement already satisfied: numpy in c:\\users\\aakansh\\appdata\\roaming\\python\\python312\\site-packages (from mahotas) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install mahotas\n",
    "\n",
    "import mahotas\n",
    "\n",
    "\n",
    "# SIFT\n",
    "def extract_sift_features(images, max_features=128):\n",
    "    sift = cv2.SIFT_create()\n",
    "    features = []\n",
    "    for img in images:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        if descriptors is None:\n",
    "            # If no descriptors are found, use a zero vector\n",
    "            descriptors = np.zeros((1, max_features))\n",
    "        # Flatten descriptors and truncate or pad to max_features\n",
    "        flattened = descriptors.flatten()\n",
    "        if len(flattened) < max_features:\n",
    "            # Pad with zeros if fewer features are found\n",
    "            flattened = np.pad(flattened, (0, max_features - len(flattened)), mode='constant')\n",
    "        else:\n",
    "            # Truncate to the first max_features\n",
    "            flattened = flattened[:max_features]\n",
    "        features.append(flattened)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# HOG\n",
    "def extract_hog_features(images):\n",
    "    from skimage.feature import hog\n",
    "    features = []\n",
    "    for img in images:\n",
    "        hog_features = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                           cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "        features.append(hog_features)\n",
    "    return np.array(features)\n",
    "\n",
    "# LBP\n",
    "def extract_lbp_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        lbp = mahotas.features.lbp(img, radius=1, points=8, ignore_zeros=False)\n",
    "        features.append(lbp)\n",
    "    return np.array(features)\n",
    "\n",
    "# Combine features\n",
    "sift_features = extract_sift_features(X_train_processed)\n",
    "hog_features = extract_hog_features(X_train_processed)\n",
    "lbp_features = extract_lbp_features(X_train_processed)\n",
    "\n",
    "X_train_features = np.hstack((sift_features, hog_features, lbp_features))\n",
    "\n",
    "# Repeat for test data\n",
    "sift_test_features = extract_sift_features(X_test_processed)\n",
    "hog_test_features = extract_hog_features(X_test_processed)\n",
    "lbp_test_features = extract_lbp_features(X_test_processed)\n",
    "\n",
    "X_test_features = np.hstack((sift_test_features, hog_test_features, lbp_test_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02018c4-8e00-423e-a479-81ca882b9ce1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141462,
     "status": "ok",
     "timestamp": 1738012352441,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "b02018c4-8e00-423e-a479-81ca882b9ce1",
    "outputId": "520a44dc-f237-4743-826d-870558a331f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.65      0.70       248\n",
      "           1       0.70      0.75      0.73       238\n",
      "           2       0.79      0.86      0.82       209\n",
      "\n",
      "    accuracy                           0.75       695\n",
      "   macro avg       0.75      0.75      0.75       695\n",
      "weighted avg       0.75      0.75      0.74       695\n",
      "\n",
      "Accuracy: 0.7453237410071942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create and train the model\n",
    "model = LogisticRegression(max_iter=500, random_state=42)\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_features)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde916da-a31a-4472-8d82-9026a97c1a7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1738012423823,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "cde916da-a31a-4472-8d82-9026a97c1a7e",
    "outputId": "376f679c-d8d4-45da-fe6e-467e0b5e9abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: ['Lung_Opacity']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Preprocess and extract features for a new image\n",
    "def preprocess_and_extract(image):\n",
    "    image = preprocess_image(image)\n",
    "    sift_features = extract_sift_features([image])\n",
    "    hog_features = extract_hog_features([image])\n",
    "    lbp_features = extract_lbp_features([image])\n",
    "    return np.hstack((sift_features, hog_features, lbp_features))\n",
    "\n",
    "# Load a new image\n",
    "new_image = cv2.imread('../Sample_Test/LO/3.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "new_image = cv2.resize(new_image, (128, 128))\n",
    "new_features = preprocess_and_extract(new_image)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(new_features)\n",
    "print(\"Predicted Class:\", label_encoder.inverse_transform(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f63bb723-3a55-4c01-82d0-47496d920ab9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1738012582595,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "f63bb723-3a55-4c01-82d0-47496d920ab9",
    "outputId": "0a382f68-50f4-4479-e501-d86d7e5a7918"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/My Drive/Lung X-Ray Project/Models/modelv1.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import joblib\n",
    "\n",
    "# # Save the trained Logistic Regression model\n",
    "# # content/drive/My Drive/Lung X-Ray Project/Sample_Test/VN/3.jpg\n",
    "# joblib.dump(model, '/content/drive/My Drive/Lung X-Ray Project/Models/modelv1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "XbNcETa-oCFY",
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1738012692264,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "XbNcETa-oCFY"
   },
   "outputs": [],
   "source": [
    "# modelv1 = joblib.load('/content/drive/My Drive/Lung X-Ray Project/Models/modelv1.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "AwUP-1-IoYGO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1738012759477,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "AwUP-1-IoYGO",
    "outputId": "a29ee0aa-09b1-419e-d73d-33b3cf885292"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# modellol = joblib.load('/content/drive/My Drive/Lung X-Ray Project/Models/logistic_regression.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vkhXOvLCoL66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1738012818615,
     "user": {
      "displayName": "Lol",
      "userId": "16578275113680997970"
     },
     "user_tz": -330
    },
    "id": "vkhXOvLCoL66",
    "outputId": "3fce641e-d917-433a-94eb-9bfe12db730c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: ['Viral Pneumonia']\n",
      "Predicted Class 2 : ['Viral Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Preprocess and extract features for a new image\n",
    "# def preprocess_and_extract(image):\n",
    "#     image = preprocess_image(image)\n",
    "#     sift_features = extract_sift_features([image])\n",
    "#     hog_features = extract_hog_features([image])\n",
    "#     lbp_features = extract_lbp_features([image])\n",
    "#     return np.hstack((sift_features, hog_features, lbp_features))\n",
    "\n",
    "# # Load a new image\n",
    "# new_image = cv2.imread('/content/drive/My Drive/Lung X-Ray Project/Sample_Test/VN/3.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "# new_image = cv2.resize(new_image, (128, 128))\n",
    "# new_features = preprocess_and_extract(new_image)\n",
    "\n",
    "# # Predict\n",
    "# prediction = modelv1.predict(new_features)\n",
    "# prediction2 = modellol.predict(new_features)\n",
    "\n",
    "# print(\"Predicted Class:\", label_encoder.inverse_transform(prediction))\n",
    "# print(\"Predicted Class 2 :\", label_encoder.inverse_transform(prediction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_k4LEdViomxL",
   "metadata": {
    "id": "_k4LEdViomxL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
