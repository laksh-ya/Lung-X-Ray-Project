{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc9fbb-e03d-4150-9673-8b7fbc117238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment: Aug=False, KP=23\n",
      "Experiment Aug=False, KP=23: Accuracy=0.8932, Best Params={'C': 0.001, 'max_iter': 1000}, Time=103.8s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Lung_Disease       0.92      0.91      0.91       418\n",
      "      Normal       0.85      0.86      0.86       247\n",
      "\n",
      "    accuracy                           0.89       665\n",
      "   macro avg       0.89      0.89      0.89       665\n",
      "weighted avg       0.89      0.89      0.89       665\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Running experiment: Aug=True, KP=23\n"
     ]
    }
   ],
   "source": [
    "# Version 13 – Enhanced Data Augmentation\n",
    "# -----------------------------------------\n",
    "# Changes compared to Version 4 Final:\n",
    "# 1. Data Augmentation: For each training image, we add three augmented variants:\n",
    "#    - Horizontal flip,\n",
    "#    - Rotation by +10 degrees,\n",
    "#    - Rotation by -10 degrees.\n",
    "#    This increases the effective training set size.\n",
    "# 2. The rest of the pipeline is identical to Version 4:\n",
    "#    - Use original 128x128 grayscale images.\n",
    "#    - Feature Extraction:\n",
    "#         * SIFT Fixed: Top 23 keypoints (sorted by response) flattened.\n",
    "#         * SIFT Average: Average SIFT descriptor (128-d).\n",
    "#         * HOG: Standard parameters (9 orientations, 8x8 cell, 2x2 block, L2-Hys).\n",
    "#         * LBP: Normalized histogram (radius=2, 16 points, bins 0–10).\n",
    "#    - Features are concatenated and scaled using StandardScaler.\n",
    "#    - Logistic Regression is tuned using GridSearchCV (solver lbfgs).\n",
    "# 3. No PCA is applied (since PCA previously hurt performance).\n",
    "# -----------------------------------------\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from skimage.feature import hog\n",
    "import mahotas\n",
    "import time\n",
    "\n",
    "#########################\n",
    "# Data Loading Function\n",
    "#########################\n",
    "def load_data(base_path='../Database/'):\n",
    "    data, labels = [], []\n",
    "    # Load Normal images\n",
    "    normal_path = os.path.join(base_path, \"Normal\")\n",
    "    for img_name in os.listdir(normal_path):\n",
    "        img_path = os.path.join(normal_path, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        data.append(img)\n",
    "        labels.append(\"Normal\")\n",
    "    # Merge Lung Disease images from \"Lung_Opacity\" and \"Viral Pneumonia\"\n",
    "    lung_folders = [\"Lung_Opacity\", \"Viral Pneumonia\"]\n",
    "    for folder in lung_folders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            data.append(img)\n",
    "            labels.append(\"Lung_Disease\")\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "#########################\n",
    "# Data Augmentation Functions\n",
    "#########################\n",
    "def horizontal_flip(image):\n",
    "    return cv2.flip(image, 1)\n",
    "\n",
    "def rotate_image(image, angle):\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return rotated\n",
    "\n",
    "#########################\n",
    "# Feature Extraction Functions\n",
    "#########################\n",
    "def extract_sift_fixed(images, num_keypoints=23):\n",
    "    sift = cv2.SIFT_create()\n",
    "    features = []\n",
    "    for img in images:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        if descriptors is None or len(keypoints) == 0:\n",
    "            feat = np.zeros(num_keypoints * 128)\n",
    "        else:\n",
    "            kp_desc = list(zip(keypoints, descriptors))\n",
    "            kp_desc.sort(key=lambda x: x[0].response, reverse=True)\n",
    "            selected = kp_desc[:num_keypoints]\n",
    "            descs = [d for kp, d in selected]\n",
    "            if len(descs) < num_keypoints:\n",
    "                pad = [np.zeros(128) for _ in range(num_keypoints - len(descs))]\n",
    "                descs.extend(pad)\n",
    "            feat = np.hstack(descs)\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_sift_avg(images):\n",
    "    sift = cv2.SIFT_create()\n",
    "    features = []\n",
    "    for img in images:\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "        if descriptors is None or len(descriptors) == 0:\n",
    "            feat = np.zeros(128)\n",
    "        else:\n",
    "            feat = np.mean(descriptors, axis=0)\n",
    "        features.append(feat)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_hog_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        hog_feat = hog(img, orientations=9, pixels_per_cell=(8, 8),\n",
    "                       cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "        features.append(hog_feat)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_lbp_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        lbp = mahotas.features.lbp(img, radius=2, points=16, ignore_zeros=False)\n",
    "        hist, _ = np.histogram(lbp, bins=np.arange(0, 11), density=True)\n",
    "        features.append(hist)\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_features(images, num_keypoints=23):\n",
    "    sift_fixed = extract_sift_fixed(images, num_keypoints=num_keypoints)\n",
    "    sift_avg = extract_sift_avg(images)\n",
    "    hog_feats = extract_hog_features(images)\n",
    "    lbp_feats = extract_lbp_features(images)\n",
    "    sift_combined = np.hstack((sift_fixed, sift_avg))\n",
    "    return np.hstack((sift_combined, hog_feats, lbp_feats))\n",
    "\n",
    "#########################\n",
    "# Experiment Pipeline\n",
    "#########################\n",
    "def run_experiment(augmentation=False, num_keypoints=23):\n",
    "    # Load data\n",
    "    X, y = load_data()\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "    \n",
    "    # Augmentation (for training images only)\n",
    "    if augmentation:\n",
    "        X_train_aug = []\n",
    "        y_train_aug = []\n",
    "        for img, label in zip(X_train, y_train):\n",
    "            X_train_aug.append(img)\n",
    "            y_train_aug.append(label)\n",
    "            # Add horizontal flip\n",
    "            X_train_aug.append(horizontal_flip(img))\n",
    "            y_train_aug.append(label)\n",
    "            # Add rotation +10 degrees\n",
    "            X_train_aug.append(rotate_image(img, 10))\n",
    "            y_train_aug.append(label)\n",
    "            # Add rotation -10 degrees\n",
    "            X_train_aug.append(rotate_image(img, -10))\n",
    "            y_train_aug.append(label)\n",
    "        X_train = np.array(X_train_aug)\n",
    "        y_train = np.array(y_train_aug)\n",
    "    \n",
    "    # Use original images for feature extraction\n",
    "    X_train_proc = X_train\n",
    "    X_test_proc = X_test\n",
    "    \n",
    "    # Extract features\n",
    "    X_train_features = extract_features(X_train_proc, num_keypoints=num_keypoints)\n",
    "    X_test_features = extract_features(X_test_proc, num_keypoints=num_keypoints)\n",
    "    \n",
    "    # Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "    X_test_scaled = scaler.transform(X_test_features)\n",
    "    \n",
    "    # Model training with grid search\n",
    "    param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  'max_iter': [1000, 2000, 3000]}\n",
    "    grid = GridSearchCV(LogisticRegression(random_state=42, solver='lbfgs'),\n",
    "                        param_grid, cv=5, n_jobs=-1)\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    model = grid.best_estimator_\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "    return acc, best_params, report, scaler, label_encoder\n",
    "\n",
    "#########################\n",
    "# Run Experiments\n",
    "#########################\n",
    "experiments = [\n",
    "    {\"augmentation\": False, \"num_keypoints\": 23},\n",
    "    {\"augmentation\": True, \"num_keypoints\": 23},\n",
    "    {\"augmentation\": False, \"num_keypoints\": 25},\n",
    "    {\"augmentation\": True, \"num_keypoints\": 25},\n",
    "    {\"augmentation\": False, \"num_keypoints\": 30},\n",
    "    {\"augmentation\": True, \"num_keypoints\": 30},\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for exp in experiments:\n",
    "    key = f\"Aug={exp['augmentation']}, KP={exp['num_keypoints']}\"\n",
    "    print(f\"Running experiment: {key}\")\n",
    "    start = time.time()\n",
    "    acc, best_params, report, scaler, label_encoder = run_experiment(\n",
    "        augmentation=exp['augmentation'],\n",
    "        num_keypoints=exp['num_keypoints']\n",
    "    )\n",
    "    duration = time.time() - start\n",
    "    results[key] = {\"accuracy\": acc, \"best_params\": best_params, \"report\": report, \"time\": duration}\n",
    "    print(f\"Experiment {key}: Accuracy={acc:.4f}, Best Params={best_params}, Time={duration:.1f}s\")\n",
    "    print(report)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"Summary of Experiments:\")\n",
    "for key, res in results.items():\n",
    "    print(f\"{key}: Accuracy={res['accuracy']:.4f}, Best Params={res['best_params']}, Time={res['time']:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab3569-4e7f-48b0-96dc-58ffb2a9872d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c450473-809e-4545-bc68-004918492462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2a889-54be-4664-b118-486892ffb55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c8506-b2ed-4a9a-a9ef-643c41e83c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a30d6d-16f0-4ba4-921e-a287909c3558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00155b-de69-473f-b3d1-157f74ed30cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc15ee7-9ef3-46c9-8535-001df5756fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff897ab6-b9fc-4e79-80b7-3a3bc9bacdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c88af3-1ef7-4001-bb92-cff22acec0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf933a1-b2ef-4d4c-a097-81300e07b378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd20a68-b393-4d20-b4e2-f903d4305fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d8423-e679-40fb-b9e0-b5762208eace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3babdb-bdd4-45e9-aef0-3b32388718bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758f016-0546-4551-a3c9-079e47eb550b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a7de6-75da-407c-b5c9-645f7db92ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d1d6e-5de9-4cfa-9749-02f7a5147e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838b4e6-415c-4011-9b23-1ffdfd962a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e22b8-a73c-46db-906f-416ceecdd5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33444ff1-c407-4dfa-a241-6cfd56d79b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883dac7-b1bf-4cbf-b39a-d1ed7be9931a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4bcd2-af3e-4fd6-9afd-f8126f019327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdbbef-8310-4a09-88cd-931bf5a5069d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16751282-0d9c-4e98-b3b7-6944b18501f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a2982-092c-49f3-be1d-0422718df007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5bdac-2217-4632-8f9d-1929ac509939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5018d052-b6f3-4586-a5db-c8ae5b79a526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
